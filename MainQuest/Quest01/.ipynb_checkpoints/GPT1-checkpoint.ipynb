{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c08e7e0e-22b9-4036-9459-87ad9a07199c",
   "metadata": {},
   "source": [
    "# Transformerì™€ ë¹„êµí•´ ë³€ê²½ì´ í•„ìš”í•œ ë¶€ë¶„\n",
    "- ì¸ì½”ë” ë¸”ë¡ ì œê±°\n",
    "  - GPT-1: ë””ì½”ë”ë¯¼ ì‚¬ìš©\n",
    "- ì…ë ¥ embedding ì²˜ë¦¬ ë³€ê²½\n",
    "  - word embedding+ positional embeddingìœ¼ë¡œ ë³€ê²½\n",
    "- ë””ì½”ë” ë¸”ë¡ ìˆ˜ì •\n",
    "  - Masked Self-Attention ì ìš©\n",
    "  - Encoder-Decoder Attention ì œê±°\n",
    "- ë””ì½”ë” ì¶œë ¥ í›„ Transposed Token Embedding + Softmaxë¡œ ì˜ˆì¸¡\n",
    "- Loss ê³„ì‚° ë°©ì‹ ë³€ê²½\n",
    "  - ë§¤ ìŠ¤í…ë§ˆë‹¤ cross entropy(next token prediction loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f320657-8d99-49a3-a884-d74a4b03021b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e4ea5f-41f8-42ef-a35b-0dde4655a99b",
   "metadata": {},
   "source": [
    "# 1. ë°ì´í„° ì „ì²˜ë¦¬\n",
    "- Q, Aë¥¼ í•˜ë‚˜ì˜ ì‹œí€¸ìŠ¤ë¡œ ë§Œë“¤ê¸°, êµ¬ë¶„ìë¡œ êµ¬ë¶„í•˜ê¸°\n",
    "- í† í¬ë‚˜ì´ì¦ˆ\n",
    "-  input/target ì‹œí€¸ìŠ¤ ìƒì„±\n",
    "    - Context Window ë°©ì‹: tokenized ëœ ì‹œí€¸ìŠ¤ë¥¼ ì¼ì •í•œ í¬ê¸°(max_len)ì˜ windowë¡œ ì˜ë¼ì„œ, ê° window ë§ˆë‹¤ ë‹¤ìŒ í† í° ì˜ˆì¸¡ì„ í•™ìŠµí•˜ê²Œ í•¨. \n",
    "    - Input = ì²« í† í°ë¶€í„° N-1ë²ˆì§¸ê¹Œì§€ / Target = ë‘ ë²ˆì§¸ í† í°ë¶€í„° Në²ˆì§¸ê¹Œì§€  \n",
    "-  ë°°ì¹˜ ë§Œë“¤ê¸°\n",
    "    - max_len ë¡œ í¬ê¸° ë§ì¶”ê¸°  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68472b55-3dd5-4929-a444-af23bc029996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7908</th>\n",
       "      <td>ì¡ì•„ì•¼ ë˜ëŠ”ê±´ì§€ ë§ì•„ì•¼ ë˜ëŠ”ê±´ì§€ ëª¨ë¥´ê² ë„¤.</td>\n",
       "      <td>í›„íšŒí•  ê±°ë¼ë©´ ë§ˆì§€ë§‰ìœ¼ë¡œ ì¡ëŠ” ê±´ ì–´ë–¨ê¹Œìš”.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10786</th>\n",
       "      <td>ì˜¤ëŠ˜ ì§ë‚¨ ë§Œë‚˜ëŠ”ë° ì˜·ì´ ì—†ë‹¤.</td>\n",
       "      <td>ì´ ê¹€ì— í•˜ë‚˜ ì¥ë§Œí•˜ëŠ” ê±´ ì–´ë–¨ê¹Œìš”.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>ê±´ë¬¼ì£¼ê°€ ì§±ì¸ë°</td>\n",
       "      <td>ì´ë£° ìˆ˜ ìˆì„ ê±°ì˜ˆìš”.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6648</th>\n",
       "      <td>ìƒˆ ë‚¨ìê°€ ìƒê²¼ëŒ€</td>\n",
       "      <td>ìƒˆ ì‚¬ëŒ ë§Œë‚˜ë©´ ë¼ìš”.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4942</th>\n",
       "      <td>í•„í†µ ë‘ê³  ì˜´</td>\n",
       "      <td>ë¹Œë¦¬ë©´ ë¼ìš”.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Q                         A  label\n",
       "7908   ì¡ì•„ì•¼ ë˜ëŠ”ê±´ì§€ ë§ì•„ì•¼ ë˜ëŠ”ê±´ì§€ ëª¨ë¥´ê² ë„¤.  í›„íšŒí•  ê±°ë¼ë©´ ë§ˆì§€ë§‰ìœ¼ë¡œ ì¡ëŠ” ê±´ ì–´ë–¨ê¹Œìš”.      1\n",
       "10786        ì˜¤ëŠ˜ ì§ë‚¨ ë§Œë‚˜ëŠ”ë° ì˜·ì´ ì—†ë‹¤.      ì´ ê¹€ì— í•˜ë‚˜ ì¥ë§Œí•˜ëŠ” ê±´ ì–´ë–¨ê¹Œìš”.      2\n",
       "117                   ê±´ë¬¼ì£¼ê°€ ì§±ì¸ë°              ì´ë£° ìˆ˜ ìˆì„ ê±°ì˜ˆìš”.      0\n",
       "6648                 ìƒˆ ë‚¨ìê°€ ìƒê²¼ëŒ€              ìƒˆ ì‚¬ëŒ ë§Œë‚˜ë©´ ë¼ìš”.      1\n",
       "4942                   í•„í†µ ë‘ê³  ì˜´                   ë¹Œë¦¬ë©´ ë¼ìš”.      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ë°ì´í„° ê°€ì ¸ì˜¤ê¸°\n",
    "path = \"C:/Users/yjneo/AIFFEL_quest_rs/Exploration/Ex07/ChatbotData.csv\"\n",
    "data = pd.read_csv(path)\n",
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81b2126e-61df-4d99-a833-de9a7276267a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# def preprocess_sentence(sentence):\n",
    "#     # 1. ì–‘ìª½ ê³µë°± ì œê±°\n",
    "#     sentence = sentence.strip()\n",
    "\n",
    "#     # 2. ì£¼ìš” êµ¬ë‘ì (?, !, ,)ì€ ë„ì–´ì“°ê¸°ë¡œ ë¶„ë¦¬ (ë§ˆì¹¨í‘œëŠ” ì œê±°)\n",
    "#     sentence = re.sub(r\"([?!,])\", r\" \\1 \", sentence)\n",
    "\n",
    "#     # 3. í•œê¸€, ì˜ë¬¸, ìˆ«ì, ì£¼ìš” êµ¬ë‘ì (?, !, ,)ë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ëŠ” ì œê±°\n",
    "#     sentence = re.sub(r\"[^ê°€-í£a-zA-Z0-9?!,]\", \" \", sentence)\n",
    "\n",
    "#     # 4. ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ ì¤„ì´ê¸°\n",
    "#     sentence = re.sub(r\"\\s+\", \" \", sentence).strip()\n",
    "\n",
    "#     return sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1212d7cf-5f4c-47c1-b1df-2323b8bc0daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ì§ˆë¬¸ì„ questions, ë‹µë³€ì„ answersì— ì €ì¥í•©ë‹ˆë‹¤.\n",
    "questions_raw= list(data['Q'])\n",
    "answers_raw = list(data['A'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163005d7-288c-4b3e-9a48-0f518abfd545",
   "metadata": {},
   "source": [
    "## Q, Aë¥¼ í•˜ë‚˜ì˜ ì‹œí€¸ìŠ¤ë¡œ ë§Œë“¤ê¸°, êµ¬ë¶„ìë¡œ êµ¬ë¶„í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb86267e-ae1e-4daa-9f7d-e985341ab8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12ì‹œ ë•¡! <delim> í•˜ë£¨ê°€ ë˜ ê°€ë„¤ìš”. <delim> 1ì§€ë§ í•™êµ ë–¨ì–´ì¡Œì–´ <delim> ìœ„ë¡œí•´ ë“œë¦½ë‹ˆë‹¤. <delim> 3ë°•4ì¼ ë†€ëŸ¬ê°€ê³  ì‹¶ë‹¤ <delim> ì—¬í–‰ì€ ì–¸ì œë‚˜\n"
     ]
    }
   ],
   "source": [
    "# êµ¬ë¶„ì ì •ì˜\n",
    "delimiter = '<delim>'\n",
    "\n",
    "# Q, Aë¥¼ êµ¬ë¶„ìë¡œ ì´ì–´ë¶™ì¸ í•˜ë‚˜ì˜ ê¸´ ì‹œí€€ìŠ¤ë¥¼ ë§Œë“ ë‹¤\n",
    "long_sequence = ''\n",
    "\n",
    "for question, answer in zip(questions_raw, answers_raw):\n",
    "    # ì§ˆë¬¸ê³¼ ë‹µë³€ì„ êµ¬ë¶„ìë¡œ ì´ì–´ë¶™ì„\n",
    "    pair = f'{question} {delimiter} {answer}'\n",
    "    # ìŒ“ì•„ë‚˜ê°€ê¸°\n",
    "    if long_sequence:\n",
    "        long_sequence += f' {delimiter} {pair}'\n",
    "    else:\n",
    "        long_sequence = pair\n",
    "\n",
    "# ê²°ê³¼ í™•ì¸\n",
    "print(long_sequence[:100])  # ì•ë¶€ë¶„ 5ìë§Œ ë¯¸ë¦¬ í™•ì¸"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc082dc-7413-4c0a-85ad-05fa6497db92",
   "metadata": {},
   "source": [
    "## í† í¬ë‚˜ì´ì§•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd449184-0b9e-4745-882c-0f31dcd38aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens after tokenization: 542598\n",
      "[1254, 1255, 40, 1237, 758, 1238, 1237, 1265, 1305, 1306, 1313, 1310, 1314, 1267, 1237, 5, 271, 7, 1237, 223, 1237, 7, 29, 1, 1251, 1237, 1265, 1305, 1306, 1313]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "# 1. Tokenizer í•™ìŠµ: ê¸´ ì‹œí€€ìŠ¤ë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ ì„œë¸Œì›Œë“œ í† í¬ë‚˜ì´ì € ìƒì„±\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    long_sequence, target_vocab_size=2**13)\n",
    "\n",
    "\n",
    "# 2. ë‹¨ì–´ì¥ í¬ê¸°\n",
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "\n",
    "\n",
    "# 3. ê¸´ ì‹œí€€ìŠ¤ ì •ìˆ˜ ì¸ì½”ë”© (Tokenize)\n",
    "tokenized_ids = tokenizer.encode(long_sequence)\n",
    "\n",
    "print(f\"Total tokens after tokenization: {len(tokenized_ids)}\")\n",
    "print(tokenized_ids[:30])  # ì•ìª½ ì¼ë¶€ í† í° í™•ì¸\n",
    "\n",
    "\n",
    "#\n",
    "# #ì‹œì‘í† í°ê³¼ ì¢…ë£Œí† í°ì— ê³ ìœ í•œ ì •ìˆ˜ ë¶€ì—¬\n",
    "# START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "# VOCAB_SIZE = tokenizer.vocab_size + 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17962a05-7012-43c4-9e0d-81d2f64c9536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì •ìˆ˜ ì¸ì½”ë”©, íŒ¨ë”©, ìƒ˜í”Œ ì œê±° í•¨ìˆ˜\n",
    "# ìƒ˜í”Œì˜ ìµœëŒ€ í—ˆìš© ê¸¸ì´ ë˜ëŠ” íŒ¨ë”© í›„ì˜ ìµœì¢… ê¸¸ì´\n",
    "MAX_LENGTH = 40\n",
    "# print(MAX_LENGTH)\n",
    "\n",
    "# ì •ìˆ˜ ì¸ì½”ë”©, ìµœëŒ€ ê¸¸ì´ë¥¼ ì´ˆê³¼í•˜ëŠ” ìƒ˜í”Œ ì œê±°, íŒ¨ë”©\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "  tokenized_inputs, tokenized_outputs = [], []\n",
    "  \n",
    "  for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "    # ì •ìˆ˜ ì¸ì½”ë”© ê³¼ì •ì—ì„œ ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í°ì„ ì¶”ê°€\n",
    "    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "\n",
    "    # ìµœëŒ€ ê¸¸ì´ 40 ì´í•˜ì¸ ê²½ìš°ì—ë§Œ ë°ì´í„°ì…‹ìœ¼ë¡œ í—ˆìš©\n",
    "    if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
    "      tokenized_inputs.append(sentence1)\n",
    "      tokenized_outputs.append(sentence2)\n",
    "  \n",
    "  # # ìµœëŒ€ ê¸¸ì´ 21ìœ¼ë¡œ ëª¨ë“  ë°ì´í„°ì…‹ì„ íŒ¨ë”©\n",
    "  # tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "  #     tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  # tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "  #     tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  \n",
    "  return tokenized_inputs, tokenized_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a926d22-990f-49fe-a2a0-f02ae7677378",
   "metadata": {},
   "source": [
    "## input/target ì‹œí€¸ìŠ¤ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0729786-2d5e-49ea-8214-4cd61de094a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ ìƒì„±ëœ context windows ê°œìˆ˜: 2118\n",
      "ì²« ë²ˆì§¸ input: [1254, 1255, 40, 1237, 758, 1238, 1237, 1265, 1305, 1306]\n",
      "ì²« ë²ˆì§¸ target: [1255, 40, 1237, 758, 1238, 1237, 1265, 1305, 1306, 1313]\n"
     ]
    }
   ],
   "source": [
    "max_len = 512\n",
    "# context window ë°©ì‹ í•¨ìˆ˜\n",
    "def create_context_windows(tokenized_ids, max_len, stride=None):\n",
    "    \"\"\"\n",
    "    ê¸´ tokenized_idsë¥¼ context window ë°©ì‹ìœ¼ë¡œ ìë¥´ëŠ” í•¨ìˆ˜\n",
    "    Args:\n",
    "        tokenized_ids: ì „ì²´ ê¸´ í† í° ID ë¦¬ìŠ¤íŠ¸\n",
    "        max_len: window í¬ê¸°\n",
    "        stride: ìŠ¬ë¼ì´ë”©í•  ê°„ê²© (ê¸°ë³¸ì€ max_lenê³¼ ê°™ìŒ = overlap ì—†ìŒ)\n",
    "    Returns:\n",
    "        input_ids_list, target_ids_list\n",
    "    \"\"\"\n",
    "    if stride is None:\n",
    "        stride = max_len  # ê²¹ì¹¨ ì—†ì´ ë°”ë¡œ ë‹¤ìŒ windowë¡œ ë„˜ì–´ê°\n",
    "\n",
    "    input_ids_list = []\n",
    "    target_ids_list = []\n",
    "\n",
    "    for start_idx in range(0, len(tokenized_ids) - max_len, stride):\n",
    "        end_idx = start_idx + max_len\n",
    "\n",
    "        input_ids = tokenized_ids[start_idx:end_idx]\n",
    "        target_ids = tokenized_ids[start_idx + 1:end_idx + 1]\n",
    "\n",
    "        input_ids_list.append(input_ids)\n",
    "        target_ids_list.append(target_ids)\n",
    "\n",
    "    return input_ids_list, target_ids_list\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ\n",
    "input_sequences, target_sequences = create_context_windows(\n",
    "    tokenized_ids,\n",
    "    max_len=512,\n",
    "    stride=256  # ì ˆë°˜ì”© ê²¹ì¹˜ê²Œ ìë¥´ëŠ” ì˜ˆì‹œ\n",
    ")\n",
    "\n",
    "print(f\"ì´ ìƒì„±ëœ context windows ê°œìˆ˜: {len(input_sequences)}\")\n",
    "print(f\"ì²« ë²ˆì§¸ input: {input_sequences[0][:10]}\")  # ì•ìª½ 10ê°œ í† í°\n",
    "print(f\"ì²« ë²ˆì§¸ target: {target_sequences[0][:10]}\")\n",
    "# print(input_sequences.shape)\n",
    "# print(output_sequences.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5145f261-b155-4ef8-b5b7-b72f17448f7e",
   "metadata": {},
   "source": [
    "## ë°°ì¹˜ ë°ì´í„° ë§Œë“¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f735710-8d5b-4d30-adcb-07ebbdfdb87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Input Shape: (32, 512)\n",
      "Batch Target Shape: (32, 512)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def create_batches(input_sequences, target_sequences, batch_size):\n",
    "    \"\"\"\n",
    "    input/target ì‹œí€€ìŠ¤ë¥¼ batchë¡œ ë¬¶ì–´ì£¼ëŠ” í•¨ìˆ˜\n",
    "    Args:\n",
    "        input_sequences: (list) ìë¥¸ input ì‹œí€€ìŠ¤ ë¦¬ìŠ¤íŠ¸\n",
    "        target_sequences: (list) ìë¥¸ target ì‹œí€€ìŠ¤ ë¦¬ìŠ¤íŠ¸\n",
    "        batch_size: (int) ë°°ì¹˜ í¬ê¸°\n",
    "    Returns:\n",
    "        batched_dataset: (tf.data.Dataset) ë°°ì¹˜ê°€ ì ìš©ëœ ë°ì´í„°ì…‹\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. ë¦¬ìŠ¤íŠ¸ë¥¼ í…ì„œë¡œ ë³€í™˜\n",
    "    inputs = tf.constant(input_sequences, dtype=tf.int32)\n",
    "    targets = tf.constant(target_sequences, dtype=tf.int32)\n",
    "\n",
    "    # 2. Dataset ê°ì²´ë¡œ ë³€í™˜\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((inputs, targets))\n",
    "\n",
    "    # 3. ì…”í”Œ(ì„ íƒ) + ë°°ì¹˜\n",
    "    dataset = dataset.shuffle(buffer_size=len(input_sequences))  # ì„ íƒì‚¬í•­\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# ì˜ˆì‹œ ì‚¬ìš©ë²•\n",
    "batch_size = 32\n",
    "batched_dataset = create_batches(input_sequences, target_sequences, batch_size)\n",
    "\n",
    "# í™•ì¸\n",
    "for batch_inputs, batch_targets in batched_dataset.take(1):\n",
    "    print(\"Batch Input Shape:\", batch_inputs.shape)\n",
    "    print(\"Batch Target Shape:\", batch_targets.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba11b46-a83b-4489-86cb-ac3bbbf9c949",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c05637f9-8e4d-47d8-8834-05183c3e20d1",
   "metadata": {},
   "source": [
    "# 2. ì…ë ¥ ë¸”ë¡ \n",
    "- ì…ë ¥ embedding ì²˜ë¦¬ ë³€ê²½\n",
    "- word embedding+ positional embeddingìœ¼ë¡œ ë³€ê²½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33f930c1-eb21-4999-a203-552b1ed85201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class GPTInputEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, d_model, max_len):\n",
    "        super(GPTInputEmbedding, self).__init__()\n",
    "        self.token_embedding = tf.keras.layers.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = tf.keras.layers.Embedding(max_len, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size, seq_len) ì •ìˆ˜ ì¸ì½”ë”©ëœ ì…ë ¥ ì‹œí€€ìŠ¤\n",
    "        Returns:\n",
    "            embeddings: (batch_size, seq_len, d_model) í† í° + í¬ì§€ì…˜ ì„ë² ë”© í•©ì³ì§„ ê²°ê³¼\n",
    "        \"\"\"\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # 1. Token Embedding\n",
    "        token_emb = self.token_embedding(x)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        # 2. Position Embedding\n",
    "        positions = tf.range(start=0, limit=seq_len, delta=1)\n",
    "        positions = self.position_embedding(positions)  # (seq_len, d_model)\n",
    "        positions = tf.expand_dims(positions, axis=0)  # (1, seq_len, d_model)ë¡œ ë§ì¶°ì¤Œ\n",
    "\n",
    "        # 3. Token Embedding + Position Embedding\n",
    "        embeddings = token_emb + positions  # broadcastingìœ¼ë¡œ í•©ì³ì§\n",
    "\n",
    "        return embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a986117-2298-41f9-be60-108a3652f0bf",
   "metadata": {},
   "source": [
    "## embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0387b22b-3a50-4e26-8d28-e9fa3753a318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding output shape: (32, 512, 768)\n"
     ]
    }
   ],
   "source": [
    "# ì„¸íŒ…\n",
    "vocab_size = VOCAB_SIZE   # ì•„ê¹Œ tokenizer vocab size\n",
    "d_model = 768             # GPT-1ì€ hidden size 768\n",
    "max_len = 512             # ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´\n",
    "\n",
    "# ë ˆì´ì–´ ë§Œë“¤ê¸°\n",
    "input_embedder = GPTInputEmbedding(vocab_size, d_model, max_len)\n",
    "\n",
    "# input ì‹œí€€ìŠ¤ ì˜ˆì‹œ (batch_size=2, seq_len=10)\n",
    "sample_input = tf.constant([[5, 20, 15, 47, 2, 0, 0, 0, 0, 0],\n",
    "                            [7, 18, 13, 49, 4, 3, 0, 0, 0, 0]])\n",
    "\n",
    "# ì„ë² ë”© ê²°ê³¼\n",
    "embeddings = input_embedder(batch_inputs)\n",
    "\n",
    "print(\"Embedding output shape:\", embeddings.shape)\n",
    "# (batch_size=2, seq_len=10, d_model=768)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ff5503-1276-4416-a02e-aefc0c080f9c",
   "metadata": {},
   "source": [
    "# 3. GPT ëª¨ë¸ êµ¬ì„±\n",
    "- - ë””ì½”ë” ë¸”ë¡ ìˆ˜ì •\n",
    "  - Masked Self-Attention ì ìš©\n",
    "  - Encoder-Decoder Attention ì œê±°\n",
    "- Transformer Decoderë¥¼ 12ê°œ ìŒ“ëŠ”ë‹¤. \n",
    "- model.summary\n",
    "- model.fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757a4720-979c-4af7-ba26-83c940015ce6",
   "metadata": {},
   "source": [
    "## decoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3881f365-1604-4950-b9c8-ccbf04235911",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "    super(MultiHeadAttention, self).__init__(name=name)\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "\n",
    "    assert d_model % self.num_heads == 0\n",
    "\n",
    "    # d_modelì„ num_headsë¡œ ë‚˜ëˆˆ ê°’.\n",
    "    # ë…¼ë¬¸ ê¸°ì¤€ : 64\n",
    "    self.depth = d_model // self.num_heads\n",
    "\n",
    "    # WQ, WK, WVì— í•´ë‹¹í•˜ëŠ” ë°€ì§‘ì¸µ ì •ì˜\n",
    "    self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    # WOì— í•´ë‹¹í•˜ëŠ” ë°€ì§‘ì¸µ ì •ì˜\n",
    "    self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "  # num_heads ê°œìˆ˜ë§Œí¼ q, k, vë¥¼ splití•˜ëŠ” í•¨ìˆ˜\n",
    "  def split_heads(self, inputs, batch_size):\n",
    "    inputs = tf.reshape(\n",
    "        inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "\n",
    "  def call(self, query, key, value, mask):\n",
    "    batch_size = tf.shape(query)[0]\n",
    "\n",
    "    query = self.query_dense(query)\n",
    "    key = self.key_dense(key)\n",
    "    value = self.value_dense(value)\n",
    "\n",
    "    query = self.split_heads(query, batch_size)\n",
    "    key = self.split_heads(key, batch_size)\n",
    "    value = self.split_heads(value, batch_size)\n",
    "\n",
    "    scaled_attention, _ = scaled_dot_product_attention(query, key, value, mask)\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "    concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
    "    outputs = self.dense(concat_attention)\n",
    "\n",
    "    return outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "981a760f-9486-40fc-b27a-65dc4afa14e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    if mask is not None:\n",
    "        # âœ… tf.Tensor íƒ€ì…ì¸ mask ì²˜ë¦¬\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "175092ce-50de-47d2-bfa4-557d3efc26d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë””ì½”ë”ì˜ ì²«ë²ˆì§¸ ì„œë¸Œì¸µ(sublayer)ì—ì„œ ë¯¸ë˜ í† í°ì„ Maskí•˜ëŠ” í•¨ìˆ˜\n",
    "# í˜„ì¬ê¹Œì§€ ë³¸ ì •ë³´ê¹Œì§€ë§Œ attention \n",
    "def create_look_ahead_mask(x):\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)  # (seq_len, seq_len)\n",
    "    mask = tf.expand_dims(mask, axis=0)                                # (1, seq_len, seq_len)\n",
    "    mask = tf.expand_dims(mask, axis=1)                                # (1, 1, seq_len, seq_len)\n",
    "    return mask  # broadcastingì€ ì•Œì•„ì„œ ì´ë¤„ì§\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5682a0a9-6a3c-4897-b899-47203f2e8779",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b7071e9-b594-41e3-ad4f-f44074d7cce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_layer(dff, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")  # (batch_size, seq_len, d_model)\n",
    "    look_ahead_mask = tf.keras.Input(shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "\n",
    "    # 1. Masked Multi-Head Self Attention\n",
    "    attn_output = MultiHeadAttention(d_model, num_heads, name=\"masked_attention\")(inputs, inputs, inputs, look_ahead_mask)\n",
    "    attn_output = tf.keras.layers.Dropout(rate=dropout)(attn_output)\n",
    "    out1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(inputs + attn_output)  # Add & Norm\n",
    "\n",
    "    # 2. Position-wise Feed Forward Network\n",
    "    ffn_output = tf.keras.layers.Dense(dff, activation='relu')(out1)\n",
    "    ffn_output = tf.keras.layers.Dense(d_model)(ffn_output)\n",
    "    ffn_output = tf.keras.layers.Dropout(rate=dropout)(ffn_output)\n",
    "    out2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(out1 + ffn_output)  # Add & Norm\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, look_ahead_mask],\n",
    "        outputs=out2,\n",
    "        name=name\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e1e0fe-1286-4cbf-bac1-2fbd51abd94d",
   "metadata": {},
   "source": [
    "## decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ae90d364-c44a-4fb7-8d4a-09aa1825d202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(num_layers, dff, d_model, num_heads, dropout, name='decoder'):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name='inputs')  # Embeddingëœ ì…ë ¥\n",
    "    look_ahead_mask = tf.keras.Input(shape=(1, None, None), name='look_ahead_mask')\n",
    "\n",
    "    outputs = inputs  # ì´ˆê¸°ê°’ì€ embedding ê²°ê³¼\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        outputs = decoder_layer(\n",
    "            dff=dff, d_model=d_model, num_heads=num_heads,\n",
    "            dropout=dropout, name=f'decoder_layer_{i}'\n",
    "        )([outputs, look_ahead_mask])\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, look_ahead_mask],\n",
    "        outputs=outputs,\n",
    "        name=name\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4ed983-bf59-4622-8d70-c84d4cb1bb2c",
   "metadata": {},
   "source": [
    "## ëª¨ë¸ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f63dbae8-4459-4e34-b718-0d34fe8aee99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def create_gpt1_model(vocab_size, num_layers, dff, d_model, num_heads, max_len, dropout=0.1):\n",
    "    input_ids = tf.keras.Input(shape=(None,), dtype=tf.int32, name='input_ids')  # (batch_size, seq_len)\n",
    "\n",
    "    # Embedding\n",
    "    input_embedder = GPTInputEmbedding(vocab_size, d_model, max_len)\n",
    "    embedded_inputs = input_embedder(input_ids)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "    # Look-ahead mask (Lambdaë¡œ ìƒì„±)\n",
    "    look_ahead_mask = tf.keras.layers.Lambda(\n",
    "        create_look_ahead_mask,\n",
    "        output_shape=(1, None, None),  # (batch, 1, seq_len, seq_len)\n",
    "        name='look_ahead_mask'\n",
    "    )(input_ids)\n",
    "\n",
    "    # ë””ì½”ë” í†µê³¼\n",
    "    decoder_model = decoder(\n",
    "        num_layers=num_layers,\n",
    "        dff=dff,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout\n",
    "    )\n",
    "    decoder_outputs = decoder_model([embedded_inputs, look_ahead_mask])\n",
    "\n",
    "    # Final Linear Projection + Softmax\n",
    "    lm_logits = tf.keras.layers.Dense(vocab_size, name=\"lm_head\")(decoder_outputs)\n",
    "    probs = tf.keras.layers.Softmax(axis=-1, name=\"softmax_output\")(lm_logits)\n",
    "\n",
    "    model = tf.keras.Model(inputs=input_ids, outputs=probs, name=\"gpt1_model\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9345d18-ba93-4c2e-920e-d652b5a67c76",
   "metadata": {},
   "source": [
    "## ëª¨ë¸ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c86ee6d4-252d-4a90-87a2-764c938db7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\yjneo\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:219: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\yjneo\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:219: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gpt1_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gpt1_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                  </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape              </span>â”ƒ<span style=\"font-weight: bold\">         Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to               </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                          â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ gpt_input_embedding_1         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)         â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,515,264</span> â”‚ input_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GPTInputEmbedding</span>)           â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ look_ahead_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)     â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ input_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ decoder (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)         â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">85,054,464</span> â”‚ gpt_input_embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ look_ahead_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lm_head (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1461</span>)        â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,123,509</span> â”‚ decoder[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ softmax_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Softmax</span>)      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1461</span>)        â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ lm_head[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_ids (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              â”‚               \u001b[38;5;34m0\u001b[0m â”‚ -                          â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ gpt_input_embedding_1         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)         â”‚       \u001b[38;5;34m1,515,264\u001b[0m â”‚ input_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            â”‚\n",
       "â”‚ (\u001b[38;5;33mGPTInputEmbedding\u001b[0m)           â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ look_ahead_mask (\u001b[38;5;33mLambda\u001b[0m)      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)     â”‚               \u001b[38;5;34m0\u001b[0m â”‚ input_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ decoder (\u001b[38;5;33mFunctional\u001b[0m)          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)         â”‚      \u001b[38;5;34m85,054,464\u001b[0m â”‚ gpt_input_embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34mâ€¦\u001b[0m â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ look_ahead_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lm_head (\u001b[38;5;33mDense\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1461\u001b[0m)        â”‚       \u001b[38;5;34m1,123,509\u001b[0m â”‚ decoder[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ softmax_output (\u001b[38;5;33mSoftmax\u001b[0m)      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1461\u001b[0m)        â”‚               \u001b[38;5;34m0\u001b[0m â”‚ lm_head[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">87,693,237</span> (334.52 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m87,693,237\u001b[0m (334.52 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">87,693,237</span> (334.52 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m87,693,237\u001b[0m (334.52 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ì„¸íŒ…\n",
    "vocab_size = VOCAB_SIZE  # ë„¤ê°€ ë§Œë“  tokenizer vocab size\n",
    "d_model = 768\n",
    "dff = 3072   # position-wise feed-forward hidden size\n",
    "num_heads = 12  # ë©€í‹° ì–´í…ì…˜ í—¤ë“œ 12ê°œ\n",
    "num_layers = 12 # ë””ì½”ë” ë ˆì´ì–´ 12ê°œ \n",
    "max_len = 512\n",
    "\n",
    "# ëª¨ë¸ ë§Œë“¤ê¸°\n",
    "gpt1_model = create_gpt1_model(\n",
    "    vocab_size=vocab_size,\n",
    "    num_layers=num_layers,\n",
    "    dff=dff,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    max_len=max_len,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# ëª¨ë¸ êµ¬ì¡° ë³´ê¸°\n",
    "gpt1_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2bf80e-dbae-4c62-bc6e-1e02b780bdd6",
   "metadata": {},
   "source": [
    "# 4. ëª¨ë¸ ì»´íŒŒì¼, í•™ìŠµ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f58c05-0fa1-4f82-b901-2a4d170221e1",
   "metadata": {},
   "source": [
    "## ì†ì‹¤ í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7a515163-f226-4462-b074-7fd14d5006b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=False, reduction='none')(y_true, y_pred)\n",
    "\n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "    loss = tf.multiply(loss, mask)\n",
    "\n",
    "    return tf.reduce_mean(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec14fb9-1f01-4d58-a247-6907a076a036",
   "metadata": {},
   "source": [
    "## ì»¤ìŠ¤í…€ëœ í•™ìŠµë¥ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "acae133b-f4cb-4c08-9027-2a350eaefde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    step = tf.cast(step, tf.float32)\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffc8798-67c4-46aa-9110-c4f415b0605f",
   "metadata": {},
   "source": [
    "## ë©”íŠ¸ë¦­"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f4cb6e57-34ed-4dd8-85ed-9c9b5521880b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "41817e68-16e3-4dce-a530-b41fb738fea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Learning Rate ìŠ¤ì¼€ì¤„ëŸ¬ ë§Œë“¤ê¸°\n",
    "learning_rate = CustomSchedule(d_model=768)\n",
    "\n",
    "# 2. Optimizer ì„¤ì • (Adam)\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=learning_rate,  # ğŸ¯ ì—¬ê¸° ìŠ¤ì¼€ì¤„ ê°ì²´ ë„£ìŒ\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-8\n",
    ")\n",
    "\n",
    "# 3. Loss í•¨ìˆ˜ ì„¤ì •\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "\n",
    "\n",
    "\n",
    "# 4. ëª¨ë¸ ì»´íŒŒì¼\n",
    "gpt1_model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "01bd742f-c904-43ed-ab2e-095a3033b815",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ì½œë°± ì •ì˜ \n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath='best_model.weights.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "callbacks = [early_stopping, checkpoint]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6f5af19e-b88d-4b45-a574-f34901f4d8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(batch_size, seq_len):\n",
    "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)  # (seq_len, seq_len)\n",
    "    look_ahead_mask = tf.expand_dims(look_ahead_mask, axis=0)                      # (1, seq_len, seq_len)\n",
    "    look_ahead_mask = tf.tile(look_ahead_mask, [batch_size, 1, 1])                  # (batch_size, seq_len, seq_len)\n",
    "    look_ahead_mask = tf.expand_dims(look_ahead_mask, axis=1)                      # (batch_size, 1, seq_len, seq_len)\n",
    "    return look_ahead_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b634ce20-7621-4b68-a07b-5ee491f5887e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m  3/120\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m2:49:50\u001b[0m 87s/step - accuracy: 0.0022 - loss: 7.3011"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: Exception ignored in sys.unraisablehook: <built-in function unraisablehook>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\yjneo\\anaconda3\\Lib\\site-packages\\ipykernel\\iostream.py\", line 694, in write\n",
      "    self._schedule_flush()\n",
      "  File \"C:\\Users\\yjneo\\anaconda3\\Lib\\site-packages\\ipykernel\\iostream.py\", line 590, in _schedule_flush\n",
      "    self.pub_thread.schedule(_schedule_in_thread)\n",
      "  File \"C:\\Users\\yjneo\\anaconda3\\Lib\\site-packages\\ipykernel\\iostream.py\", line 267, in schedule\n",
      "    self._event_pipe.send(b\"\")\n",
      "  File \"C:\\Users\\yjneo\\anaconda3\\Lib\\site-packages\\zmq\\sugar\\socket.py\", line 701, in send\n",
      "    return super().send(data, flags=flags, copy=copy, track=track)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"_zmq.py\", line 1092, in zmq.backend.cython._zmq.Socket.send\n",
      "  File \"_zmq.py\", line 1140, in zmq.backend.cython._zmq.Socket.send\n",
      "  File \"_zmq.py\", line 1339, in zmq.backend.cython._zmq._send_copy\n",
      "  File \"_zmq.py\", line 160, in zmq.backend.cython._zmq._check_rc\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m EPOCHS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# í•™ìŠµ\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m gpt1_model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m     20\u001b[0m     input_sequences,\n\u001b[0;32m     21\u001b[0m     target_sequences,\n\u001b[0;32m     22\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE,\n\u001b[0;32m     23\u001b[0m     epochs\u001b[38;5;241m=\u001b[39mEPOCHS,\n\u001b[0;32m     24\u001b[0m     validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[0;32m     25\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks\n\u001b[0;32m     26\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:371\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[0;32m    370\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 371\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m    372\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[0;32m    373\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:219\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    217\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[0;32m    218\u001b[0m     ):\n\u001b[1;32m--> 219\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m multi_step_on_iterator(iterator)\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[0;32m    221\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    879\u001b[0m     args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config\n\u001b[0;32m    880\u001b[0m )\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[0;32m    141\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall_preflattened(args)\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_flat(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[0;32m    255\u001b[0m     )\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1688\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1686\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1688\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[0;32m   1689\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1690\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   1691\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[0;32m   1692\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[0;32m   1693\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1694\u001b[0m   )\n\u001b[0;32m   1695\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1696\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1697\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1698\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1702\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1703\u001b[0m   )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# í•™ìŠµ ë°ì´í„° ì¤€ë¹„\n",
    "input_sequences = tf.convert_to_tensor(input_sequences, dtype=tf.int32)\n",
    "target_sequences = tf.convert_to_tensor(target_sequences, dtype=tf.int32)\n",
    "\n",
    "batch_size = tf.shape(input_sequences)[0]  # 1906\n",
    "seq_len = tf.shape(input_sequences)[1]     # 512\n",
    "\n",
    "# Look Ahead Mask ë§Œë“¤ê¸°\n",
    "look_ahead_mask = create_look_ahead_mask(batch_size, seq_len)\n",
    "\n",
    "# xë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ\n",
    "x = [input_sequences, look_ahead_mask]\n",
    "\n",
    "\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 10\n",
    "# í•™ìŠµ\n",
    "gpt1_model.fit(\n",
    "    input_sequences,\n",
    "    target_sequences,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_split=0.1,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "18bbc581-739e-41a3-a7ae-47c1fa3671b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# ì„¤ì¹˜ëœ GPU ë””ë°”ì´ìŠ¤ í™•ì¸\n",
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d2757335-704d-4a67-aa61-a0128065bd7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.19.0\n"
     ]
    }
   ],
   "source": [
    "# íŒŒì´ì¬ ë²„ì „ í™•ì¸\n",
    "# python --version    #3.11.9\n",
    "#tensorflow ë²„ì „ í™•ì¸\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)   # 2.19.0\n",
    "\n",
    "\n",
    "# # ì„¤ì¹˜ëœ CUDA ë²„ì „ í™•ì¸\n",
    "# nvcc --version\n",
    "# nvidia-smi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1e5404-b1f3-41cc-ad46-c778f11ae8fd",
   "metadata": {},
   "source": [
    "# ëª¨ë¸ í™•ì¸\n",
    "- ì…ë ¥ì— ë”°ë¥¸ ì¶œë ¥ì´ ìƒì„±ë˜ëŠ”ì§€ í™•ì¸\n",
    "- ì¶œë ¥ ê²°ê³¼ë¬¼ì˜ ìˆ˜ì¤€ì— ìƒê´€ì—†ì´ ëª¨ë¸ì´ ì •ìƒì ìœ¼ë¡œ ë™ì‘í•˜ëŠ”ì§€ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a2a63e-abaf-4eec-ba42-0673438d53e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
