{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "617541ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.1+cu111\n",
      "1.21.4\n",
      "4.11.3\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from datasets import Dataset \n",
    "import torch\n",
    "print(torch.__version__)\n",
    "# print(tensorflow.__version__)\n",
    "print(np.__version__)\n",
    "print(transformers.__version__)\n",
    "# print(datasets.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c5ccfb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.1+cu111\n"
     ]
    }
   ],
   "source": [
    "# !pip install torch==2.3.0 --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "953e74eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.37.0\n",
      "  Using cached transformers-4.37.0-py3-none-any.whl (8.4 MB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers==4.37.0) (4.62.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.37.0) (0.5.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.37.0) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.37.0) (2021.11.10)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.9/site-packages (from transformers==4.37.0) (0.32.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.37.0) (1.21.4)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers==4.37.0) (2.26.0)\n",
      "Collecting tokenizers<0.19,>=0.14\n",
      "  Using cached tokenizers-0.15.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.37.0) (21.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers==4.37.0) (3.13.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.0) (1.1.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.0) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.0) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=20.0->transformers==4.37.0) (3.0.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.37.0) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.37.0) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.37.0) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.37.0) (2.0.8)\n",
      "Installing collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.1\n",
      "    Uninstalling tokenizers-0.21.1:\n",
      "      Successfully uninstalled tokenizers-0.21.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.52.3\n",
      "    Uninstalling transformers-4.52.3:\n",
      "      Successfully uninstalled transformers-4.52.3\n",
      "Successfully installed tokenizers-0.15.2 transformers-4.37.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers==4.37.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a403fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os._exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e9efe7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch_xla\n",
    "# import torch_xla.core.xla_model as xm\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30bef04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall torch transformers -y\n",
    "# !pip install torch==2.3.0 --index-url https://download.pytorch.org/whl/cu118\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40c4e179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers==4.40.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b9d3b9",
   "metadata": {},
   "source": [
    "# NSMC ë°ì´í„° ë¶„ì„ ë° huggingface dataset êµ¬ì„±\n",
    "- Naver Snetiment Movie Corpus task\n",
    "- klue/ber-baseë¥¼ í™œìš©í•˜ì—¬ NSMC taskë¥¼ í•˜ëŠ” ê²ƒì´ ëª©í‘œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f0fd7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'document', 'label'],\n",
      "    num_rows: 149995\n",
      "})\n",
      "Dataset({\n",
      "    features: ['id', 'document', 'label'],\n",
      "    num_rows: 49997\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# ë°ì´í„° ë‹¤ìš´ë¡œë“œ\n",
    "import urllib.request\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")\n",
    "\n",
    "# pandasë¡œ ë°ì´í„° ì½ê¸°\n",
    "train_data = pd.read_table('ratings_train.txt')\n",
    "test_data = pd.read_table('ratings_test.txt')\n",
    "\n",
    "# ê²°ì¸¡ê°’ ì œê±°\n",
    "train_data = train_data.dropna(how='any').reset_index(drop=True)\n",
    "test_data = test_data.dropna(how='any').reset_index(drop=True)\n",
    "\n",
    "# Hugging Face Datasetìœ¼ë¡œ ë³€í™˜\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "test_dataset = Dataset.from_pandas(test_data)\n",
    "\n",
    "# ë°ì´í„°ì…‹ í™•ì¸\n",
    "print(train_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb21a6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ë°ì´í„°ì…‹ì„ dict í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
    "train_dict = train_dataset.to_dict('list')\n",
    "test_dict = test_dataset.to_dict('list')\n",
    "\n",
    "# dictë¡œ ë³€í™˜í•˜ì—¬ Hugging Face Datasetìœ¼ë¡œ ë‹¤ì‹œ ë³€í™˜\n",
    "hf_train_dataset = Dataset.from_dict(train_dict)\n",
    "hf_test_dataset = Dataset.from_dict(test_dict)\n",
    "\n",
    "# train/validation split (ì„ì˜ë¡œ 20% validation ë°ì´í„°ë¡œ ì‚¬ìš©)\n",
    "train_val_dataset = hf_train_dataset.train_test_split(test_size=0.2)\n",
    "tf_train_dataset = train_val_dataset['train']\n",
    "tf_val_dataset = train_val_dataset['test']\n",
    "tf_test_dataset = hf_test_dataset  # í…ŒìŠ¤íŠ¸ ë°ì´í„°\n",
    "\n",
    "# í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ee24747",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# tokenizerì™€ model\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "huggingface_tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')\n",
    "huggingface_model = AutoModelForSequenceClassification.from_pretrained('klue/bert-base', num_labels = 2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57a1795c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers==4.37.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d954de5",
   "metadata": {},
   "source": [
    "# ë¶ˆëŸ¬ì˜¨ tokenizerë¡œ ë°ì´í„°ì…‹ ì „ì²˜ë¦¬, model í•™ìŠµ ì§„í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb2583c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import numpy as np\n",
    "# from transformers import AutoTokenizer\n",
    "# from datasets import Dataset\n",
    "\n",
    "# # í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "# tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')\n",
    "\n",
    "# def convert_examples_to_features(examples, labels, max_seq_len, tokenizer):\n",
    "#     encodings = tokenizer(\n",
    "#         examples,  # ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸\n",
    "#         truncation=True,  # ê¸¸ì´ê°€ max_seq_lenì„ ë„˜ìœ¼ë©´ ìë¥´ê¸°\n",
    "#         padding='max_length',  # ê¸¸ì´ê°€ max_seq_len ë¯¸ë§Œì´ë©´ íŒ¨ë”©\n",
    "#         max_length=max_seq_len,  # ìµœëŒ€ ê¸¸ì´ ì„¤ì •\n",
    "#         return_tensors=None,  # í…ì„œë¥¼ ë°˜í™˜í•˜ì§€ ì•ŠìŒ, ëŒ€ì‹  ë¦¬ìŠ¤íŠ¸ë‚˜ numpy ë°°ì—´ ë°˜í™˜\n",
    "#         return_attention_mask=True,  # Attention Mask ë°˜í™˜\n",
    "#         return_token_type_ids=False  # ë‹¨ì¼ ë¬¸ì¥ ë¶„ë¥˜ì´ë¯€ë¡œ token_type_ids í•„ìš” ì—†ìŒ\n",
    "#     )\n",
    "    \n",
    "#     # í† í¬ë‚˜ì´ì¦ˆëœ ê°’ë“¤\n",
    "#     input_ids = encodings['input_ids']\n",
    "#     attention_masks = encodings['attention_mask']\n",
    "    \n",
    "#     # ë ˆì´ë¸”ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
    "#     data_labels = labels  # ì´ë¯¸ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì¡´ì¬\n",
    "\n",
    "#     # ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°˜í™˜, í…ì„œ ëŒ€ì‹  ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜\n",
    "#     return {\n",
    "#         'input_ids': input_ids,\n",
    "#         'attention_mask': attention_masks,\n",
    "#         'labels': data_labels\n",
    "#     }\n",
    "\n",
    "# # ì˜ˆì‹œ ë°ì´í„°\n",
    "# train_data = [\"ì´ ì˜í™” ì§„ì§œ ì¬ë°Œì–´ìš”\", \"ì˜í™” ë‚´ìš©ì´ ì§€ë£¨í–ˆì–´ìš”\"]  # ì˜ˆì‹œ ë¬¸ì¥\n",
    "# train_labels = [1, 0]  # ì˜ˆì‹œ ë ˆì´ë¸”: 1 = ê¸ì •, 0 = ë¶€ì •\n",
    "\n",
    "# # ë°ì´í„°ì…‹ ë³€í™˜\n",
    "# max_seq_len = 128  # ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ ì„¤ì •\n",
    "# train_dataset = Dataset.from_dict({'document': train_data, 'label': train_labels})\n",
    "\n",
    "# # ë°ì´í„°ì…‹ì— ì „ì²˜ë¦¬ ì ìš©\n",
    "# train_dataset = train_dataset.map(lambda x: convert_examples_to_features(x['document'], x['label'], max_seq_len, tokenizer), batched=True)\n",
    "\n",
    "# # ì „ì²˜ë¦¬ëœ ë°ì´í„°ì…‹ í™•ì¸\n",
    "# print(train_dataset[0])  # ì²« ë²ˆì§¸ ìƒ˜í”Œì„ í™•ì¸\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6492f1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9b375d321524fb697fea658d4cf4a8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1efdeeae057744fda6e8b0a9ab257823",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60081973674448b99bf6c1c15e78d505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 9822864, 'document': 'ëŒ€ì„œì‚¬ì‹œì˜ ì™„ë²½í•˜ê³  í™”ë ¤í•œ ë§‰', 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [2, 18979, 2063, 2067, 2079, 5537, 19521, 5725, 2470, 1037, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 1, 'label': 1}\n",
      "torch.Size([119996, 128]) torch.Size([119996, 128]) torch.Size([119996])\n",
      "torch.Size([29999, 128]) torch.Size([29999, 128]) torch.Size([29999])\n",
      "torch.Size([49997, 128]) torch.Size([49997, 128]) torch.Size([49997])\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# ì˜ˆì‹œ ë°ì´í„°\n",
    "train_data = [\"ì´ ì˜í™” ì§„ì§œ ì¬ë°Œì–´ìš”\", \"ì˜í™” ë‚´ìš©ì´ ì§€ë£¨í–ˆì–´ìš”\"]  # ì˜ˆì‹œ ë¬¸ì¥\n",
    "train_labels = [1, 0]  # ì˜ˆì‹œ ë ˆì´ë¸”: 1 = ê¸ì •, 0 = ë¶€ì •\n",
    "\n",
    "# í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')\n",
    "\n",
    "# ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜\n",
    "def convert_examples_to_features(examples, labels, max_seq_len, tokenizer):\n",
    "    # í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ë°°ì¹˜ ì²˜ë¦¬\n",
    "    encodings = tokenizer(\n",
    "        examples,  # ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸\n",
    "        truncation=True,  # ê¸¸ì´ê°€ max_seq_lenì„ ë„˜ìœ¼ë©´ ìë¥´ê¸°\n",
    "        padding='max_length',  # ê¸¸ì´ê°€ max_seq_len ë¯¸ë§Œì´ë©´ íŒ¨ë”©\n",
    "        max_length=max_seq_len,  # ìµœëŒ€ ê¸¸ì´ ì„¤ì •\n",
    "        return_tensors=None,  # í…ì„œë¥¼ ë°˜í™˜í•˜ì§€ ì•Šê³ , ë¦¬ìŠ¤íŠ¸ë‚˜ numpy ë°°ì—´ë¡œ ë°˜í™˜\n",
    "        return_attention_mask=True,  # Attention Mask ë°˜í™˜\n",
    "        return_token_type_ids=False  # ë‹¨ì¼ ë¬¸ì¥ ë¶„ë¥˜ì´ë¯€ë¡œ token_type_ids í•„ìš” ì—†ìŒ\n",
    "    )\n",
    "    \n",
    "    # tokenizedëœ ê°’ë“¤\n",
    "    input_ids = encodings['input_ids']\n",
    "    attention_masks = encodings['attention_mask']\n",
    "    \n",
    "    # ë ˆì´ë¸”ì„ í…ì„œë¡œ ë³€í™˜\n",
    "    data_labels = labels  # ì´ë¯¸ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì¡´ì¬\n",
    "\n",
    "    # ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°˜í™˜\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_masks,\n",
    "        'labels': data_labels\n",
    "    }\n",
    "\n",
    "# ë°ì´í„°ì…‹ ì „ì²˜ë¦¬\n",
    "max_seq_len = 128  # ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ ì„¤ì •\n",
    "\n",
    "# tf_train_dataset, tf_val_dataset, tf_test_datasetì— ì „ì²˜ë¦¬ ì ìš©\n",
    "tf_train_dataset = tf_train_dataset.map(\n",
    "    lambda x: convert_examples_to_features(x['document'], x['label'], max_seq_len, tokenizer),\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "tf_val_dataset = tf_val_dataset.map(\n",
    "    lambda x: convert_examples_to_features(x['document'], x['label'], max_seq_len, tokenizer),\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "tf_test_dataset = tf_test_dataset.map(\n",
    "    lambda x: convert_examples_to_features(x['document'], x['label'], max_seq_len, tokenizer),\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "# ì „ì²˜ë¦¬ëœ ë°ì´í„°ì…‹ í™•ì¸ (ì²« ë²ˆì§¸ ìƒ˜í”Œ í™•ì¸)\n",
    "print(tf_train_dataset[0])  # ì²« ë²ˆì§¸ ìƒ˜í”Œì„ í™•ì¸\n",
    "\n",
    "# í…ì„œë¡œ ë³€í™˜\n",
    "train_input_ids = torch.tensor(tf_train_dataset['input_ids'])\n",
    "train_attention_mask = torch.tensor(tf_train_dataset['attention_mask'])\n",
    "train_labels = torch.tensor(tf_train_dataset['labels'])\n",
    "\n",
    "val_input_ids = torch.tensor(tf_val_dataset['input_ids'])\n",
    "val_attention_mask = torch.tensor(tf_val_dataset['attention_mask'])\n",
    "val_labels = torch.tensor(tf_val_dataset['labels'])\n",
    "\n",
    "test_input_ids = torch.tensor(tf_test_dataset['input_ids'])\n",
    "test_attention_mask = torch.tensor(tf_test_dataset['attention_mask'])\n",
    "test_labels = torch.tensor(tf_test_dataset['labels'])\n",
    "\n",
    "# í…ì„œ í˜•íƒœë¡œ ì˜ ë³€í™˜ë˜ì—ˆëŠ”ì§€ í™•ì¸\n",
    "print(train_input_ids.shape, train_attention_mask.shape, train_labels.shape)\n",
    "print(val_input_ids.shape, val_attention_mask.shape, val_labels.shape)\n",
    "print(test_input_ids.shape, test_attention_mask.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3faedce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n",
    "# urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")\n",
    "\n",
    "# train_data = pd.read_table('ratings_train.txt')\n",
    "# test_data = pd.read_table('ratings_test.txt')\n",
    "\n",
    "# train_data = train_data.dropna(how = 'any')\n",
    "# train_data = train_data.reset_index(drop=True)\n",
    "# print(train_data.isnull().values.any())\n",
    "\n",
    "# test_data = test_data.dropna(how = 'any')\n",
    "# test_data = test_data.reset_index(drop=True)\n",
    "# print(test_data.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00012cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34b051e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = BertTokenizer.from_pretrained(\"klue/bert-base\")\n",
    "\n",
    "# def convert_examples_to_features(examples,labels,max_seq_len,tokenizer):\n",
    "\n",
    "#   input_ids = []\n",
    "#   attention_masks = []\n",
    "#   token_type_ids = []\n",
    "#   data_labels = []\n",
    "\n",
    "#   for example, label in tqdm(zip(examples,labels),total=len(examples)):\n",
    "\n",
    "#     input_id = tokenizer.encode(example,max_length=max_seq_len,pad_to_max_length=True)\n",
    "#     padding_count = input_id.count(tokenizer.pad_token_id)\n",
    "#     attention_mask = [1] * (max_seq_len - padding_count) + [0] * padding_count\n",
    "#     token_type_id = [0] * max_seq_len\n",
    "\n",
    "#     assert len(input_id) == max_seq_len, \"Error with input length {} vs {}\".format(len(input_id), max_seq_len)\n",
    "#     assert len(attention_mask) == max_seq_len, \"Error with attention mask length {} vs {}\".format(len(attention_mask), max_seq_len)\n",
    "#     assert len(token_type_id) == max_seq_len, \"Error with token type length {} vs {}\".format(len(token_type_id), max_seq_len)\n",
    "\n",
    "#     input_ids.append(input_id)\n",
    "#     attention_masks.append(attention_mask)\n",
    "#     token_type_ids.append(token_type_id)\n",
    "#     data_labels.append(label)\n",
    "\n",
    "#   input_ids = torch.tensor(input_ids)\n",
    "#   attention_masks = torch.tensor(attention_masks)\n",
    "#   token_type_ids = torch.tensor(token_type_ids)\n",
    "\n",
    "#   data_labels = torch.tensor(data_labels)\n",
    "\n",
    "#   return (input_ids, attention_masks, token_type_ids), data_labels\n",
    "\n",
    "# max_seq_len = 128\n",
    "\n",
    "# train_X,train_y = convert_examples_to_features(tf_train_dataset[\"document\"],tf_train_dataset[\"label\"],max_seq_len = max_seq_len, tokenizer = tokenizer)\n",
    "# val_X, val_y = convert_examples_to_features(tf_val_dataset[\"document\"],tf_val_dataset[\"label\"],max_seq_len = max_seq_len,tokenizer=tokenizer)\n",
    "# test_X, test_y = convert_examples_to_features(tf_test_dataset[\"document\"],tf_test_dataset[\"label\"],max_seq_len = max_seq_len,tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "\n",
    "# from tqdm import tqdm\n",
    "# import torch\n",
    "\n",
    "# def convert_examples_to_features(examples, labels, max_seq_len, tokenizer):\n",
    "#     input_ids = []\n",
    "#     attention_masks = []\n",
    "#     data_labels = []\n",
    "\n",
    "#     # í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ë°°ì¹˜ ì²˜ë¦¬\n",
    "#     encodings = tokenizer(\n",
    "#         examples,  # ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸\n",
    "#         truncation=True,  # ê¸¸ì´ê°€ max_seq_lenì„ ë„˜ìœ¼ë©´ ìë¥´ê¸°\n",
    "#         padding='max_length',  # ê¸¸ì´ê°€ max_seq_len ë¯¸ë§Œì´ë©´ íŒ¨ë”©\n",
    "#         max_length=max_seq_len,  # ìµœëŒ€ ê¸¸ì´ ì„¤ì •\n",
    "#         return_tensors='pt',  # í…ì„œë¡œ ë°˜í™˜ (PyTorch)\n",
    "#         return_attention_mask=True,  # Attention Mask ë°˜í™˜\n",
    "#         return_token_type_ids=False  # ë‹¨ì¼ ë¬¸ì¥ ë¶„ë¥˜ì´ë¯€ë¡œ token_type_ids í•„ìš” ì—†ìŒ\n",
    "#     )\n",
    "\n",
    "#     # input_ids, attention_mask, labelsë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
    "#     input_ids = encodings['input_ids']\n",
    "#     attention_masks = encodings['attention_mask']\n",
    "#     data_labels = torch.tensor(labels)  # ë ˆì´ë¸”ì„ í…ì„œë¡œ ë³€í™˜\n",
    "\n",
    "#     return (input_ids, attention_masks), data_labels\n",
    "\n",
    "# # ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ ì„¤ì •\n",
    "# max_seq_len = 128\n",
    "\n",
    "# # ì „ì²˜ë¦¬ ì‹¤í–‰\n",
    "# train_X, train_y = convert_examples_to_features(\n",
    "#     tf_train_dataset[\"document\"], \n",
    "#     tf_train_dataset[\"label\"], \n",
    "#     max_seq_len=max_seq_len, \n",
    "#     tokenizer=huggingface_tokenizer\n",
    "# )\n",
    "# val_X, val_y = convert_examples_to_features(\n",
    "#     tf_val_dataset[\"document\"], \n",
    "#     tf_val_dataset[\"label\"], \n",
    "#     max_seq_len=max_seq_len,\n",
    "#     tokenizer=huggingface_tokenizer\n",
    "# )\n",
    "# test_X, test_y = convert_examples_to_features(\n",
    "#     tf_test_dataset[\"document\"], \n",
    "#     tf_test_dataset[\"label\"], \n",
    "#     max_seq_len=max_seq_len, \n",
    "#     tokenizer=huggingface_tokenizer\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc10c396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# def convert_examples_to_features(examples, labels, max_seq_len, tokenizer):\n",
    "#     # ì…ë ¥ê°’ì„ ë‹´ì„ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”\n",
    "#     input_ids = []\n",
    "#     attention_masks = []\n",
    "#     data_labels = []\n",
    "    \n",
    "#     # í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ë°°ì¹˜ ì²˜ë¦¬\n",
    "#     encodings = tokenizer(\n",
    "#         examples,  # ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸\n",
    "#         truncation=True,  # ê¸¸ì´ê°€ max_seq_lenì„ ë„˜ìœ¼ë©´ ìë¥´ê¸°\n",
    "#         padding='max_length',  # ê¸¸ì´ê°€ max_seq_len ë¯¸ë§Œì´ë©´ íŒ¨ë”©\n",
    "#         max_length=max_seq_len,  # ìµœëŒ€ ê¸¸ì´ ì„¤ì •\n",
    "#         return_tensors='pt',  # í…ì„œë¡œ ë°˜í™˜ (PyTorch)\n",
    "#         return_attention_mask=True,  # Attention Mask ë°˜í™˜\n",
    "#         return_token_type_ids=False  # ë‹¨ì¼ ë¬¸ì¥ ë¶„ë¥˜ì´ë¯€ë¡œ token_type_ids í•„ìš” ì—†ìŒ\n",
    "#     )\n",
    "    \n",
    "#     # tokenizedëœ ê°’ë“¤\n",
    "#     input_ids = encodings['input_ids']\n",
    "#     attention_masks = encodings['attention_mask']\n",
    "    \n",
    "#     # ë ˆì´ë¸”ì„ í…ì„œë¡œ ë³€í™˜\n",
    "#     data_labels = torch.tensor(labels)  # ë ˆì´ë¸”ì„ í…ì„œë¡œ ë³€í™˜\n",
    "    \n",
    "#     # ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°˜í™˜\n",
    "#     return {\n",
    "#         'input_ids': input_ids,\n",
    "#         'attention_mask': attention_masks,\n",
    "#         'labels': data_labels\n",
    "#     }\n",
    "\n",
    "# # ì˜ˆì‹œ ë°ì´í„°\n",
    "# max_seq_len = 128  # ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´\n",
    "# train_data = [\"ì´ ì˜í™” ì§„ì§œ ì¬ë°Œì–´ìš”\", \"ì˜í™” ë‚´ìš©ì´ ì§€ë£¨í–ˆì–´ìš”\"]  # ì˜ˆì‹œ ë¬¸ì¥\n",
    "# train_labels = [1, 0]  # ì˜ˆì‹œ ë ˆì´ë¸”: 1 = ê¸ì •, 0 = ë¶€ì •\n",
    "\n",
    "# # í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "# from transformers import AutoTokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')\n",
    "\n",
    "# # ì „ì²˜ë¦¬ í•¨ìˆ˜ í˜¸ì¶œ\n",
    "# features = convert_examples_to_features(train_data, train_labels, max_seq_len, tokenizer)\n",
    "\n",
    "# # ì „ì²˜ë¦¬ëœ ë°ì´í„° ì¶œë ¥\n",
    "# print(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93783c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install accelerate -U\n",
    "# !pip install transformers[torch]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7aec2767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.40.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.9/site-packages (4.40.1)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.52.3-py3-none-any.whl (10.5 MB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers) (2021.11.10)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.9/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from transformers) (1.21.4)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /opt/conda/lib/python3.9/site-packages (from transformers) (0.32.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Collecting tokenizers<0.22,>=0.21\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->transformers) (2.0.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers) (2.10)\n",
      "Installing collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.19.1\n",
      "    Uninstalling tokenizers-0.19.1:\n",
      "      Successfully uninstalled tokenizers-0.19.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.40.1\n",
      "    Uninstalling transformers-4.40.1:\n",
      "      Successfully uninstalled transformers-4.40.1\n",
      "Successfully installed tokenizers-0.21.1 transformers-4.52.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import transformers\n",
    "print(transformers.__version__)\n",
    "!pip install --upgrade transformers\n",
    "\n",
    "output_dir = './transformers2'\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir,                                         # outputì´ ì €ì¥ë  ê²½ë¡œ\n",
    "    evaluation_strategy=\"epoch\",           #evaluationí•˜ëŠ” ë¹ˆë„\n",
    "    learning_rate = 2e-5,                         #learning_rate\n",
    "    per_device_train_batch_size = 8,   # ê° device ë‹¹ batch size\n",
    "    per_device_eval_batch_size = 8,    # evaluation ì‹œì— batch size\n",
    "    num_train_epochs = 3,                     # train ì‹œí‚¬ ì´ epochs\n",
    "    weight_decay = 0.01,                        # weight decay\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1bdafd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "metric_accuracy = load_metric(\"accuracy\")\n",
    "metric_f1 = load_metric(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)  # ì˜ˆì¸¡ê°’ì„ ë¼ë²¨ë¡œ ë³€í™˜ (ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ í´ë˜ìŠ¤ ì„ íƒ)\n",
    "    \n",
    "    # ì •í™•ë„ì™€ F1-score ê³„ì‚°\n",
    "    accuracy = metric_accuracy.compute(predictions=predictions, references=labels)\n",
    "    f1 = metric_f1.compute(predictions=predictions, references=labels)\n",
    "    \n",
    "    return {\"accuracy\": accuracy[\"accuracy\"], \"f1\": f1[\"f1\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5bd3f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transformers\n",
    "# print(transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7122f280",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='45000' max='45000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [45000/45000 3:42:04, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.294000</td>\n",
       "      <td>0.286232</td>\n",
       "      <td>0.897997</td>\n",
       "      <td>0.897563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.222200</td>\n",
       "      <td>0.374714</td>\n",
       "      <td>0.904863</td>\n",
       "      <td>0.903854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.163600</td>\n",
       "      <td>0.458725</td>\n",
       "      <td>0.903263</td>\n",
       "      <td>0.903150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ~\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=huggingface_model,           # í•™ìŠµì‹œí‚¬ model\n",
    "    args=training_arguments,           # TrainingArgumentsì„ í†µí•´ ì„¤ì •í•œ arguments\n",
    "    train_dataset=tf_train_dataset,    # training dataset\n",
    "    eval_dataset=tf_val_dataset,       # evaluation dataset\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()\n",
    "print(\"ìŠ~\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b5ca05",
   "metadata": {},
   "source": [
    "- accuracy :90%ì´ìƒ\n",
    "- f1-score 90% ì´ìƒ\n",
    "- training lossëŠ” ê¾¸ì¤€íˆ ê°ì†Œí•˜ê³  ìˆë‹¤. 0.2940 -> 0.1630\n",
    "- validation lossëŠ” ì¦ê°€. 0.2862 -> 0.4487\n",
    "- Accuracy / F1 ScoreëŠ” ì•½ê°„ ì¦ê°€ í›„ ì•½ê°„ í•˜ë½\n",
    "\n",
    "    Accuracy:\n",
    "\n",
    "        Epoch 1 â†’ 2: ìƒìŠ¹ (0.8979 â†’ 0.9048)\n",
    "\n",
    "        Epoch 2 â†’ 3: í•˜ë½ (0.9048 â†’ 0.9032)\n",
    "\n",
    "    F1 Score:\n",
    "\n",
    "        Epoch 1 â†’ 2: ìƒìŠ¹ (0.8975 â†’ 0.9038)\n",
    "\n",
    "        Epoch 2 â†’ 3: í•˜ë½ (0.9038 â†’ 0.9031)\n",
    "\n",
    "â†’ ì„±ëŠ¥ì´ ì •ì²´ëœ ìƒíƒœì´ë©°, ê³¼ì í•© ì˜ì‹¬\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bfe9c9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ë§ˆì§€ë§‰ ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¡œë“œ (ì˜ˆ: checkpoint-5000)\n",
    "# model = BertForSequenceClassification.from_pretrained('./transformers/checkpoint-5000')\n",
    "\n",
    "\n",
    "# í•™ìŠµ ì™„ë£Œ í›„ ëª¨ë¸ ì €ì¥\n",
    "trainer.save_model('./basemodel2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b627fb2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6250/6250 08:27]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.46605202555656433,\n",
       " 'eval_accuracy': 0.9026541592495549,\n",
       " 'eval_f1': 0.9044017992182436,\n",
       " 'eval_runtime': 508.4451,\n",
       " 'eval_samples_per_second': 98.333,\n",
       " 'eval_steps_per_second': 12.292,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(tf_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fb8066",
   "metadata": {},
   "source": [
    "- Accuracy/F1 90% ì´ìƒìœ¼ë¡œ ìš°ìˆ˜\n",
    "- eval loss 0.46ìœ¼ë¡œ ì•½ê°„ ë†’ì€ í¸, ì•½ê°„ì˜ overfitting\n",
    "- validationê³¼ testì—ì„œ ëª¨ë‘ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì„"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99506b02",
   "metadata": {},
   "source": [
    "# fine-tuningì„ í†µí•˜ì—¬ ëª¨ë¸ accuracy í–¥ìƒì‹œí‚¤ê¸°\n",
    "- early stopping ì ìš©\n",
    "- learning rate scheduler ì‚¬ìš©\n",
    "- learning rate ê°ì†Œ\n",
    "- dropout ì¡°ì •\n",
    "- mixed precision traing  fp16 = trueë¡œ í•™ìŠµì†ë„ ì˜¬ë¦¬ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b4ab7bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… GPU memory cleared\n"
     ]
    }
   ],
   "source": [
    "#ë©”ëª¨ë¦¬ ì´ˆê¸°í™”\n",
    "import torch, gc\n",
    "\n",
    "def reset_cuda():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"âœ… GPU memory cleared\")\n",
    "\n",
    "reset_cuda()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c565464d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EarlyStoppingCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "88992926",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained('klue/bert-base', num_labels=2)\n",
    "config.hidden_dropout_prob = 0.3  # ê¸°ë³¸ì€ 0.1 â†’ ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•´ ì¦ê°€\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'klue/bert-base', \n",
    "    config=config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fb4f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35125' max='75000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [35125/75000 1:45:00 < 1:59:12, 5.57 it/s, Epoch 4.68/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.389800</td>\n",
       "      <td>0.276043</td>\n",
       "      <td>0.889363</td>\n",
       "      <td>0.888874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.280700</td>\n",
       "      <td>0.283571</td>\n",
       "      <td>0.897463</td>\n",
       "      <td>0.897936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.240500</td>\n",
       "      <td>0.261236</td>\n",
       "      <td>0.904430</td>\n",
       "      <td>0.903295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.215100</td>\n",
       "      <td>0.331682</td>\n",
       "      <td>0.902430</td>\n",
       "      <td>0.903415</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fine_tune_results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size =16,   # ê° device ë‹¹ batch size 8 -> 16ë¡œ \n",
    "    per_device_eval_batch_size = 16,  \n",
    "    learning_rate=1e-5,  \n",
    "    lr_scheduler_type = 'cosine',                       #cosine scheduler ì¶”ê°€\n",
    "    warmup_ratio = 0.1,                            #ì „ì²´ stepì˜ 10%\n",
    "    num_train_epochs=10,                          # ğŸ’¡ EarlyStoppingì´ ìˆìœ¼ë¯€ë¡œ ë” í¬ê²Œ ì„¤ì • ê°€ëŠ¥\n",
    "    weight_decay=0.01,                           # ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ l2 legularization\n",
    "    load_best_model_at_end=True,                 # âœ… EarlyStoppingê³¼ í•¨ê»˜ ì‚¬ìš©\n",
    "    metric_for_best_model=\"accuracy\",           # ì¡°ê¸° ì¢…ë£Œ ê¸°ì¤€ ë©”íŠ¸ë¦­\n",
    "    greater_is_better=True,                     # ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ (accuracyì˜ ê²½ìš°)\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    fp16=True     #í•™ìŠµ ì†ë„ ë†’ì´ê¸°, ë©”ëª¨ë¦¬ ì‚¬ìš© ì¤„ì´ê¸°\n",
    ")\n",
    " \n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tf_train_dataset,\n",
    "    eval_dataset=tf_val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]  # âœ… patience: 2 epoch\n",
    ")\n",
    "trainer.train()\n",
    "trainer.save_model('./fine_tuned_model')\n",
    "trainer.evaluate(tf_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfc51ba",
   "metadata": {},
   "source": [
    "- train/val lossê°€ ì•ˆì •ì ìœ¼ë¡œ ì¤„ì–´ë“ ë‹¤. \n",
    "- accuracy/f1ì´ 90% ì´ìƒìœ¼ë¡œ ìš°ìˆ˜\n",
    "- epoch 3 ì´í›„ accuracyê°€ ë–¨ì–´ì§€ê³  train/val loss ì¦ê°€\n",
    "- epoch ìˆ˜ëŠ” 3ì„ ë„˜ì§€ ì•ŠëŠ” ê²ƒì´ ì¢‹ì•„ë³´ì„"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc364cd5",
   "metadata": {},
   "source": [
    "# Bucketingì„ ì ìš©í•˜ì—¬ í•™ìŠµì‹œí‚¤ê³  ì´ì „ ëª¨ë¸(basemodel)ê³¼ ë¹„êµ\n",
    "- íŒ¨ë”©ì„ dynamicìœ¼ë¡œ ë°”ê¾¼ë‹¤ìŒì— bucketing ì ìš©\n",
    "- í…ìŠ¤íŠ¸ ê¸¸ì´ì— ë”°ë¼ ì—¬ëŸ¬ê°œì˜ bucketìœ¼ë¡œ ë‚˜ëˆ ì„œ ë¹„ìŠ·í•œ ê¸¸ì´ë¼ë¦¬ ê°™ì€ batch êµ¬ì„±\n",
    "- ëª¨ë¸ ì„±ëŠ¥ í–¥ìƒ ì¸¡ë©´\n",
    "- í›ˆë ¨ ì‹œê°„ ì¸¡ë©´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "633b2036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6f232d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… GPU memory cleared\n"
     ]
    }
   ],
   "source": [
    "#ë©”ëª¨ë¦¬ ì´ˆê¸°í™”\n",
    "import torch, gc\n",
    "\n",
    "def reset_cuda():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"âœ… GPU memory cleared\")\n",
    "\n",
    "reset_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5e156cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# ë°ì´í„° ë‹¤ìš´ë¡œë“œ\n",
    "import urllib.request\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", \"ratings_train.txt\")\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", \"ratings_test.txt\")\n",
    "\n",
    "# pandas ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
    "train_df = pd.read_table(\"ratings_train.txt\").dropna().reset_index(drop=True)\n",
    "test_df = pd.read_table(\"ratings_test.txt\").dropna().reset_index(drop=True)\n",
    "\n",
    "# HuggingFace Dataset ë³€í™˜\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9266dfef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6743bbdc7a054b3ea5de7ba83bba8bcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be775e8c4dba407dbd933e7fffbbf9fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# dynamic padding ì ìš©\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")\n",
    "\n",
    "# tokenizer í•¨ìˆ˜ ì •ì˜\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"document\"],\n",
    "        truncation=True,\n",
    "        padding=False,  # âœ… dynamic paddingì„ ìœ„í•´ False\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "\n",
    "# tokenize ì ìš©\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a6a6ad7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c50ea26dac24a49937202ff467a259d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/149995 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5aac3d28ab743128bcfa48c0b193735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/49997 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def add_length(example):\n",
    "    example[\"length\"] = sum(example[\"attention_mask\"])\n",
    "    return example\n",
    "\n",
    "train_dataset = train_dataset.map(add_length)\n",
    "test_dataset = test_dataset.map(add_length)\n",
    "\n",
    "# ì •ë ¬\n",
    "train_dataset = train_dataset.sort(\"length\")\n",
    "test_dataset = test_dataset.sort(\"length\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0500d41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "from torch.utils.data.sampler import Sampler\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8e027a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BucketBatchSampler(Sampler):\n",
    "    def __init__(self, lengths, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.sorted_indices = np.argsort(lengths)\n",
    "        self.batches = [\n",
    "            self.sorted_indices[i:i + batch_size]\n",
    "            for i in range(0, len(self.sorted_indices), batch_size)\n",
    "        ]\n",
    "\n",
    "    def __iter__(self):\n",
    "        np.random.shuffle(self.batches)\n",
    "        for batch in self.batches:\n",
    "            yield batch.tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c0f96fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, hf_dataset):\n",
    "        self.dataset = hf_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.dataset[idx][\"input_ids\"],           # âœ… ë¦¬ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ ë°˜í™˜\n",
    "            \"attention_mask\": self.dataset[idx][\"attention_mask\"],\n",
    "            \"labels\": self.dataset[idx][\"label\"],                  # â— 'label' â†’ 'labels'ë¡œ ì´ë¦„ë§Œ í†µì¼\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6a2b5549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = train_dataset.rename_column(\"label\", \"labels\")\n",
    "# test_dataset = test_dataset.rename_column(\"label\", \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9e5436e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader ìƒì„±\n",
    "\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# Dataset ìƒì„±\n",
    "train_data = CustomDataset(train_dataset)\n",
    "test_data = CustomDataset(test_dataset)\n",
    "\n",
    "# BucketBatchSampler\n",
    "train_sampler = BucketBatchSampler(train_dataset[\"length\"], batch_size=16)\n",
    "test_sampler = BucketBatchSampler(test_dataset[\"length\"], batch_size=16)\n",
    "\n",
    "# Collator: ë™ì  íŒ¨ë”© ì ìš©\n",
    "collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_data, batch_sampler=train_sampler, collate_fn=collator)\n",
    "test_loader = DataLoader(test_data, batch_sampler=test_sampler, collate_fn=collator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "db4fd415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e125663c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.11.3\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c18d1bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "33f245c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Epoch 1 ì‹œì‘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9375/9375 [18:01<00:00,  8.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train Loss: 0.2739, Val Loss: 0.2474, Acc: 0.9002, F1: 0.9030\n",
      "âœ… Best model saved (epoch 1)\n",
      "\n",
      "ğŸ” Epoch 2 ì‹œì‘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9375/9375 [18:01<00:00,  8.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Train Loss: 0.1844, Val Loss: 0.2446, Acc: 0.9044, F1: 0.9036\n",
      "âœ… Best model saved (epoch 2)\n",
      "\n",
      "ğŸ” Epoch 3 ì‹œì‘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9375/9375 [18:01<00:00,  8.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Train Loss: 0.1215, Val Loss: 0.2773, Acc: 0.8995, F1: 0.9033\n",
      "\n",
      "ğŸ•’ ì´ í•™ìŠµ ì‹œê°„: 0ì‹œê°„ 58ë¶„ 48ì´ˆ\n",
      "\n",
      "ğŸ“Š [Test] Evaluating best model...\n",
      "[Test] Loss: 0.2446 | Acc: 0.9044 | F1: 0.9036\n",
      "ğŸ•’ Test Evaluation Time: 89.72 seconds\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AutoTokenizer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# âœ… device ì„¤ì •\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# âœ… ëª¨ë¸ ë¡œë“œ\n",
    "model = BertForSequenceClassification.from_pretrained(\"klue/bert-base\", num_labels=2).to(device)\n",
    "\n",
    "# âœ… Optimizer ì„¤ì • (Trainerì™€ ë™ì¼í•˜ê²Œ ì„¤ì •)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "\n",
    "\n",
    "\n",
    "# âœ… í‰ê°€ ì§€í‘œ í•¨ìˆ˜\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(batch[\"labels\"].cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='binary')  # binary classification\n",
    "    return total_loss / len(data_loader), acc, f1\n",
    "\n",
    "# âœ… í›ˆë ¨ í•¨ìˆ˜\n",
    "def train_epoch(model, data_loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(data_loader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "# âœ… ì „ì²´ í•™ìŠµ ë£¨í”„\n",
    "best_f1 = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(3):\n",
    "    print(f\"\\nğŸ” Epoch {epoch+1} ì‹œì‘\")\n",
    "    train_loss = train_epoch(model, train_loader)\n",
    "    val_loss, val_acc, val_f1 = evaluate(model, test_loader)\n",
    "    print(f\"[Epoch {epoch+1}] Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, F1: {val_f1:.4f}\")\n",
    "\n",
    "    # âœ… ê°€ì¥ ì¢‹ì€ ëª¨ë¸ ì €ì¥\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        torch.save(model.state_dict(), \"best_bucketing_model.pt\")\n",
    "        print(f\"âœ… Best model saved (epoch {epoch+1})\")\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# âœ… í•™ìŠµ ì‹œê°„ ì¶œë ¥\n",
    "elapsed = end_time - start_time\n",
    "hours, minutes, seconds = int(elapsed // 3600), int((elapsed % 3600) // 60), int(elapsed % 60)\n",
    "print(f\"\\nğŸ•’ ì´ í•™ìŠµ ì‹œê°„: {hours}ì‹œê°„ {minutes}ë¶„ {seconds}ì´ˆ\")\n",
    "\n",
    "# âœ… í…ŒìŠ¤íŠ¸ í‰ê°€\n",
    "print(\"\\nğŸ“Š [Test] Evaluating best model...\")\n",
    "test_start_time = time.time()\n",
    "\n",
    "model.load_state_dict(torch.load(\"best_bucketing_model.pt\"))\n",
    "test_loss, test_acc, test_f1 = evaluate(model, test_loader)\n",
    "\n",
    "test_end_time = time.time()\n",
    "test_duration = test_end_time - test_start_time\n",
    "\n",
    "print(f\"[Test] Loss: {test_loss:.4f} | Acc: {test_acc:.4f} | F1: {test_f1:.4f}\")\n",
    "print(f\"ğŸ•’ Test Evaluation Time: {test_duration:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7c4eb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4caf9cee",
   "metadata": {},
   "source": [
    "## ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ\n",
    "\n",
    "| í•­ëª©               | ì´ì „ ëª¨ë¸ (Trainer) | í˜„ì¬ ëª¨ë¸ (Custom loop + Bucketing) |\n",
    "|--------------------|----------------------|--------------------------------------|\n",
    "| **Test Loss**      | 0.4660               | 0.2446                               |\n",
    "| **Test Accuracy**  | 0.8601               | 0.9044                               |\n",
    "| **Test F1 Score**  | 0.8540               | 0.9036                               |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb72c38b",
   "metadata": {},
   "source": [
    "## í›ˆë ¨ ì‹œê°„ ë¹„êµ\n",
    "| í•­ëª©                | ì´ì „ ëª¨ë¸ (Trainer) | í˜„ì¬ ëª¨ë¸ (Custom loop + Bucketing)     |\n",
    "| ----------------- | --------------- | ----------------------------------- |\n",
    "| **ì´ Epoch ìˆ˜**     | 3            | 3| \n",
    "| **ì´ í›ˆë ¨ ì‹œê°„**       | 3ì‹œê°„ 42ë¶„      |    58ë¶„ 48ì´ˆ                 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da9eb0e",
   "metadata": {},
   "source": [
    "bucketingì˜ íš¨ê³¼\n",
    "- ëª¨ë¸ ì„±ëŠ¥ì„ ë¹„êµí•˜ì˜€ì„ ë•Œ ì´ì „ ëª¨ë¸ë³´ë‹¤ ì„±ëŠ¥ì´ ê°œì„ ë¨(accuracy, f1)\n",
    "- ì‹œê°„íš¨ìœ¨ì  ì¸¡ë©´ì—ì„œ í›ˆë ¨ì‹œê°„ì´ 3ì‹œê°„ 42ë¶„ì—ì„œ 58ë¶„ 48ì´ˆë¡œ 75%ê°ì†Œ\n",
    "- ë™ì íŒ¨ë”© + bucketing ì „ëµì„ ì‚¬ìš©í•˜ë©´ í›ˆë ¨ì†ë„ì™€ ëª¨ë¸ ì„±ëŠ¥ ëª¨ë‘ì— ê¸ì •ì ì¸ ì˜í–¥ì„ ì¤€ë‹¤ê³  í•  ìˆ˜ ìˆë‹¤. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261bf065",
   "metadata": {},
   "source": [
    "# íšŒê³ \n",
    "- í•™ìŠµ ë„ì¤‘ OOM(Out of Memory) ì—ëŸ¬ê°€ ë¹ˆë²ˆí•˜ê²Œ ë°œìƒí–ˆìœ¼ë‚˜, ëª¨ë¸ í•™ìŠµ ë° ì €ì¥ í›„ GPUë©”ëª¨ë¦¬ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì´ˆê¸°í™”í•¨ìœ¼ë¡œì¨ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆì—ˆë‹¤.\n",
    "- ë©”ëª¨ë¦¬ ìµœì í™” ë£¨í‹´(gc.collect() + torch.cuda.empty_cache())ì˜ ì¤‘ìš”ì„±ì„ ê²½í—˜í–ˆë‹¤. \n",
    "- basemodel í•™ìŠµ ì‹œ validation lossê°€ ì ì°¨ ì¦ê°€í•˜ë©° ê³¼ì í•© ì–‘ìƒì„ ë³´ì˜€ë‹¤.\n",
    "- dropout ì ìš©, learning rate ì¡°ì •, early stopping ë“±ì„ ì ìš©í•˜ì—¬ validation lossê°€ ê°ì†Œí•˜ëŠ” ê²ƒì„ í™•ì¸í•˜ì˜€ì§€ë§Œ accuracyëŠ” í¬ê²Œ í–¥ìƒë˜ì§€ ì•ŠìŒ(0.9026 -> 0.9032)\n",
    "- epochì€ 3ì„ ë„˜ì§€ ì•Šì•„ì•¼ ê³¼ì í•©ì„ ë°©ì§€í•  ìˆ˜ ìˆì—ˆë‹¤. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d615abc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
